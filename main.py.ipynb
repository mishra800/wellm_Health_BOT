{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ae1bd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_sequence' from 'tensorflow.python.util.nest' (C:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\nest.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m stemmer \u001b[38;5;241m=\u001b[39m LancasterStemmer()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtflearn\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tflearn\\__init__.py:25\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summarize, summarize_activations, \\\n\u001b[0;32m     22\u001b[0m     summarize_gradients, summarize_variables, summarize_all\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Predefined ops\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalization\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tflearn\\layers\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormalization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m batch_normalization, local_response_normalization\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m regression\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecurrent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lstm, gru, simple_rnn, bidirectional_rnn, \\\n\u001b[0;32m     12\u001b[0m     BasicRNNCell, BasicLSTMCell, GRUCell\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membedding_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m embedding\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge, merge_outputs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tflearn\\layers\\recurrent.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rnn_cell_impl \u001b[38;5;28;01mas\u001b[39;00m _rnn_cell, dynamic_rnn \u001b[38;5;28;01mas\u001b[39;00m _drnn\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core_rnn_cell\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_sequence\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'is_sequence' from 'tensorflow.python.util.nest' (C:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\nest.py)"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "import numpy\n",
    "import tflearn\n",
    "import tensorflow\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "with open('intents1.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "try:\n",
    "    with open(\"data.pickle\", \"rb\") as f:\n",
    "        words, labels, training, output = pickle.load(f) #saving these variables in the file\n",
    "except:\n",
    "    #these blank lists are created as we want to go through the json file\n",
    "    words = []\n",
    "    labels = []\n",
    "    docs_x = []\n",
    "    docs_y = []\n",
    "\n",
    "    for intent in data['intents']:\n",
    "        for pattern in intent['patterns']:\n",
    "            #tokenize the words, stemming. Root words are formed\n",
    "            wrds = nltk.word_tokenize(pattern) #returns a list of tokenized words\n",
    "            words.extend(wrds) #adds to words\n",
    "            docs_x.append(wrds)\n",
    "            #gives what intent the tag is a part of\n",
    "            docs_y.append(intent[\"tag\"])\n",
    "\n",
    "        if intent['tag'] not in labels:\n",
    "            labels.append(intent['tag'])\n",
    "\n",
    "    #removing all the duplicate elements\n",
    "    words = [stemmer.stem(w.lower()) for w in words if w != \"?\"] #removing any question marks to not have any meaning to our model, and stemming\n",
    "    words = sorted(list(set(words))) #set removes the duplicate elements\n",
    "\n",
    "    labels = sorted(labels) #sorting the labels\n",
    "\n",
    "    #create training and testing output\n",
    "    training = []\n",
    "    output = []\n",
    "\n",
    "    out_empty = [0 for _ in range(len(labels))]\n",
    "    #neural network does not understand strings, but only numbers.\n",
    "    #presenting the words into numbers\n",
    "    for x, doc in enumerate(docs_x):\n",
    "        #list to keep a check on what words are present\n",
    "        #stemming the words\n",
    "        bag = [] #bag of words\n",
    "        wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "        #going through the words and adding the information to bag\n",
    "        for w in words:\n",
    "            if w in wrds: #word exsits so add 1 to the list\n",
    "                bag.append(1)\n",
    "            else: #word does not exsit so add 0 to the list\n",
    "                bag.append(0)\n",
    "\n",
    "        output_row = out_empty[:]\n",
    "        #where the tag is in our labels, and set value to 1 in output\n",
    "        output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "        training.append(bag)\n",
    "        output.append(output_row)\n",
    "\n",
    "    #turning the lists into nparrays to be able to fed into model\n",
    "    training = numpy.array(training)\n",
    "    output = numpy.array(output)\n",
    "\n",
    "    with open(\"data.pickle\", \"wb\") as f:\n",
    "         pickle.dump((words, labels, training, output), f)\n",
    "\n",
    "#resetting the graph data, to get rid of previous settings\n",
    "tensorflow.compat.v1.reset_default_graph()\n",
    "#defines the input shape for our model\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "#8 neurons for the first hidden layer\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "#8 neurons for the second hidden layer\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "#gets probability for each neuron in the output layer,\n",
    "#the neuron which has the highest probability is selected\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\") #\n",
    "net = tflearn.regression(net)\n",
    "print(model.summary())\n",
    "#training the model\n",
    "model = tflearn.DNN(net) #deep neural network\n",
    "try:\n",
    "    x\n",
    "    model.load(\"model.tflearn\")\n",
    "except:\n",
    "#we show the model the data 1000 times, the more times it sees the data, the more accurate it should get\n",
    "    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "    model.save(\"model.tflearn\")\n",
    "\n",
    "def bag_of_words(s, words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "\n",
    "    s_words = nltk.word_tokenize(s) #list of tokenized words\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words] #stemming the words\n",
    "\n",
    "    for x in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == x: #if current word is equal to our word in the sentence, then add 1 to bag list, generates the bag of words\n",
    "                bag[i] = 1\n",
    "\n",
    "    return numpy.array(bag)\n",
    "\n",
    "\n",
    "def chat():\n",
    "    print(\"You can start talking to Bubble! If you wish to end the conversation, please type 'quit'\")\n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "        if inp.lower() == \"quit\": #way to get out of the program\n",
    "            break\n",
    "\n",
    "        results = model.predict([bag_of_words(inp, words)]) #makes prediction, this only gives us some probability, no meaningful output\n",
    "        results_index = numpy.argmax(results) #this gives the index of the tag with the greatest probability in our list\n",
    "        tag = labels[results_index] #maps the word to a particular tag\n",
    "        #if results[results_index] > 0.6:\n",
    "        for tg in data[\"intents\"]:\n",
    "            if tg['tag'] == tag:\n",
    "                responses = tg['responses']\n",
    "\n",
    "        print(random.choice(responses)) #selects a response from the tag\n",
    "\n",
    "chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa42565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27d60d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
